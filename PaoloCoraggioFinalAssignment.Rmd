---
title: "Practical Machine Learning - Final Assignment"
author: "Paolo Coraggio"
date: "27/12/2019"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('validelements.R')
library(caret)
library(rattle)
library(ggplot2)
```

# Project Outline

For the Coursera Practical Machine Learning final assignment I designed and implemented a model using data from accelerometers placed on the belt, forearm, arm, and dumbell of 6 participants of an experiment to predict the manner in which they did the exercise. 

Using the data collected in http://groupware.les.inf.puc-rio.br/har, 4 different predictive models (namely using Regression Tree, Random Forest, Bagging and Boosting algorithm) have been built and compared using a similar setting. Then, the model that resulted with higerh accuracy was further improved and, finally, used on the test dataset. 

The workflow of this project is basically:

* [loading and filtering the data](#exploratory-data-analysis)
* [creating a testing and validating test](#test-and-validation-dataset)
* [building four different models and calculating their accuracy against the validation set](#predictive-models-for-the-dataset)
* [choosing the final model](#choosing-the-model)
* [using the final model on the provodided test set](#applying-the-model-to-the-test-set)

# Exploratory data analysis

In order to build the predective model, the 

## Loading the data 

The main source of the data is http://groupware.les.inf.puc-rio.br/har. The data were downloaded in a local folder and then loaded in two different datasets: `dataset` for the training and `testset` for the testing sets

```{r, echo=FALSE}
dataset <- read.csv('./Data/pml-training.csv')
testset <- read.csv('./Data/pml-testing.csv')
```

The datasets have the following size:

```{r, echo=FALSE}
dim(dataset) 
dim(testset)
```

The `dataset` data frame consists of $19622$ datapoint of $160$ recorded features and the `testset` it's just $20$ data points. 

## Feature Extraction

The first step is to check what is the percentage of available data for each feature as the data.frame columns may contain not valid elements. I created a `validelements` function that, for each data.frame column, count their valid elements (i.e. not empty, NaN or `#DIV/0!`). I run the function on the `dataset` data.frame.

```{r, echo=FALSE, fig.align="centre", fig.height=3.0}
p.validelements <- c()
p.name <- c()

for(i in names(dataset))
{
    p.validelements <- c(p.validelements,valid.elements(dataset[[i]]))
    p.name <- c(p.name,i)
}

plot(p.validelements, pch = 3, col = "darkgrey", 
     xlab = "Feature Index", ylab = "Percentage of valid values")
```

The plot shows that data.frame variables contain or $100\%$ valide data or very few (less than $5\%$ of valide data). The features containing less the $5/%$ of data will be discharged.

Moreover, we can further exclude the first 7 features as they containg temporal information that has been chosen not to consider as the analysis is not considering a forcastin approach (that would be interesting to study further but it's out of scope the present project).  

```{r, eval=FALSE}
dataset <- dataset[, p.validelements > 0.05]
testset <- testset[, p.validelements > 0.05]

## trimming the first 7 elements

dataset <- dataset[-c(1:7)]
testset <- testset[-c(1:7)]
```

```{r, echo=FALSE}
dataset <- dataset[, p.validelements > 0.05]
testset <- testset[, p.validelements > 0.05]

dataset <- dataset[-c(1:7)]
testset <- testset[-c(1:7)]
```

The final data.frame now have the following sizes:

```{r, echo=FALSE}
dim(dataset)
dim(testset)
```

As we can see, the dataset dimension, and so its complexity, has been reduced make it also more parsimonious in its analisys.


```{r, echo=FALSE}
percentageDataset <- round(prop.table(table(dataset$classe)) * 100,1)
c1 <- cbind(freq.dataset=table(dataset$classe), percentage = percentageDataset)
```

# Test and Validation Dataset

We split the `dataset` in two, $70 \%$ of which will be used to train the different models and $30 \%$ for validating them. 

```{r}
set.seed(123)

inTrain <- createDataPartition(dataset$classe, p = 0.7, list = FALSE)
train.set <- dataset[inTrain,]
validation.set <- dataset[-inTrain,]

dim(train.set)
dim(validation.set)
```

```{r, echo=FALSE}
percentageTraining <- round(prop.table(table(train.set$classe)) * 100, 1)
c2 <- cbind(freq.training=table(train.set$classe), 
            percentage.training = percentageTraining)

```

The target variable for our analysis is the feature `classe` that shows the following distribution. 

```{r, echo=FALSE}
percentageTesting <- round(prop.table(table(validation.set$classe)) * 100,1)
c3 <- cbind(freq.validation=table(validation.set$classe), percentage.Testing = percentageTesting)
ctot <- cbind(c1, c2, c3)

knitr::kable(ctot, 
             caption = "Percentage of classes frequencies in the different datasets",
             col.names = c("Freq Dataset", "%", "Freq Training set", "%", "Freq Testing set", "%"))
```

The class distribution is almost balanced except for the Class `A` that contains a slight higher number of samples.
As we can see the Class distribuition has been preserved by the `createDataPartition` function.

# Predictive models for the dataset

This section is about how 4 different predictive models have been designed and implemented in order to chose the most promising one

## Cross Validation

As we will compare different algorithms, a preset Cross Validation parameter is set for all different models. A basic cross validation choise for this kind of dataset is 5-fold cross-validation to estimate accuracy. In order to seek a better estimate, each algorithm will be repeated 3 times.  

```{r, eval=F, echo=T}
control <- trainControl(method = "repeatedcv", 
                        number = 5, 
                        repeats = 3,
                        verboseIter = TRUE)
metric <- "Accuracy"
```

## Classification Tree

The first model is the simplest one: a classification tree. I am using the `train` function from `caret` library using `rpart` method.

```{r, echo=TRUE, eval=FALSE}
set.seed(111)

start.time <- Sys.time()
mod.CT <- train(classe ~., 
                data = train.set,
                method = 'rpart',
                tuneLength = 25,
                trControl = control,
                metric = metric)

end.time <- Sys.time()
time.takenCT <- end.time - start.time
time.takenCT

save(mod.CT, file = "modeCT.RData")
save(time.takenCT, file = "timeCT.RData")
```

With an accuracy measured on the validation set

```{r, eval = FALSE}
confusionMatrix(predict(mod.CT,newdata = validation.set),
                validation.set$classe)$overall
```

```{r, echo=FALSE, fig.align="centre"}
set.seed(111)

load("modeCT.RData")
# fancyRpartPlot(mod.CT$finalModel)
conf.matrixCT <- confusionMatrix(predict(mod.CT, 
                                         newdata = validation.set),
                                 validation.set$classe)

# round(conf.matrixCT$overall, 2)
knitr::kable(t(round(conf.matrixCT$overall, 3)))
# round(conf.matrixCT$byClass, 2)
```

## Random Forest

```{r, echo=train.set, eval=FALSE}
set.seed(111)

start.time <- Sys.time()
mod.RF <- train(classe ~.,
                data = train.set,
                method = 'rf',
                trControl = control,
                metric = metric,
                tuneLength = 25)

end.time <- Sys.time()
timeRF.taken <- end.time - start.time
timeRF.taken

save(mod.RF, file = "modRF.RData")
save(timeRF.taken, file = "timeRF.RData")
```

Gives the following accuracy:

```{r, eval=FALSE}
confusionMatrix(predict(mod.RF, validation.set), 
                validation.set$classe)$overall
```

```{r, echo=FALSE}
load("modRF.RData")
conf.matrixRF <- confusionMatrix(predict(mod.RF, validation.set), 
                validation.set$classe)

#round(conf.matrixRF$overall, 2)
knitr::kable(t(round(conf.matrixRF$overall, 3)))
# conf.matrixRF$byClass
```

## Boosting

```{r, echo=TRUE, eval=FALSE}
set.seed(111)

start.time <- Sys.time()
mod.Boosting <- train(classe ~.,
                      data = train.set,
                      method = "gbm",
                      trControl = control,
                      metric = metric,
                      verbose = FALSE)

end.time <- Sys.time()
time.Boosting <- end.time - start.time
time.Boosting
```

With accuracy:

```{r, eval=FALSE}
round(confusionMatrix(predict(mod.Boosting, validation.set), 
                validation.set$classe)$overall,3)
```

```{r, echo=FALSE}
load("modBoosting.RData")
conf.matrixBoosting <- confusionMatrix(predict(mod.Boosting, validation.set), 
                validation.set$classe)

knitr::kable(t(round(conf.matrixBoosting$overall,3)))
```

## Bagging

```{r, echo = TRUE, eval=FALSE}
set.seed(111)

start.time <- Sys.time()
mod.Bagging <- train(classe ~.,
                     data = train.set,
                     method = "treebag",
                     trControl = control,
                     metric = metric)

end.time <- Sys.time()
time.Bagging <- end.time - start.time

round(confusionMatrix(predict(mod.Bagging, validation.set), 
            validation.set$classe)$overall,3)
```

With an accuracy:

```{r, echo=FALSE}
load("modBagging.RData")
conf.matrixBagging <- confusionMatrix(predict(mod.Bagging, validation.set), 
            validation.set$classe)

knitr::kable(t(round(conf.matrixBagging$overall,3)))
# conf.matrixBagging$byClass
#plot(varImp(mod.Bagging), top =10)
```

# Choosing the model

As we can see from the accurancy measured on the testing set, the Random Forest approach looks preferible with respect the other ones. The accuracy is already quite high (about $99.450 \%$). The next model tries to tune further the Random Forest model by estimating a more approrpiate value for `ntree` and `mtry` parameters using a random search. We will use the model to estimate the parameters' tree as well.

```{r, eval=FALSE}
controlRS <- trainControl(method="repeatedcv", 
                          number=10, 
                          repeats=3, 
                          search="random",
                          verboseIter = TRUE)
set.seed(111)

rf_random <- train(classe~., 
                   data=train.set, 
                   method="rf", 
                   metric=metric, 
                   tuneLength=20, 
                   trControl=controlRS)

save(rf_random, file = "rf_random.RData")
```

```{r, echo=FALSE}
load("rf_random.RData")

plot(rf_random)
```

```{r, echo=FALSE}
plot(rf_random$finalModel)

rf_random$finalModel.legend <- if (is.null(rf_random$finalModel$test$err.rate)) {colnames(rf_random$finalModel$err.rate)} else {colnames(rf_random$finalModel$test$err.rate)}

legend("top", cex =0.7, legend=rf_random$finalModel.legend, lty=c(1,2,3,4,5), col=c(1,2,3,4,5), horiz=T)
```

The latest 2 plots suggests that a `ntree` = 100 and `mtry` = 10 should speed up the processing while assuring a better accuracy. The following is the model with these parameters and its accuracy on the testing set.

```{r, eval = FALSE}
set.seed(111)
mod.RFfinal <- train(classe ~., data = train.set,
                     method = "rf",
                     ntree = 100,
                     tuneGrid = data.frame(mtry=10),
                     trControl = trainControl(method = "repeatedcv", 
                                              number=10,
                                              repeats=3,
                                              verboseIter = TRUE))

confusionMatrix(predict(mod.RFfinal, validation.set), 
            validation.set$classe)$overall

```
```{r, echo=FALSE}
load("modRFfinal.RData")

conf.RFFinal <- confusionMatrix(predict(mod.RFfinal10, validation.set), 
            validation.set$classe)

knitr::kable(t(round(conf.RFFinal$overall,3)))
#knitr::kable(t(round(conf.RFFinal$byClass,3)))
```

And further by class.

```{r, echo=FALSE}
knitr::kable(round(conf.RFFinal$byClass, 3))
```

As we can see from the table, the improvement is really minimal in terms of accuracy although the algorithm speed is improved a lot.

The following plot shows also the top 15 features in terms of importance.

```{r, echo=FALSE}
plot(varImp(mod.RFfinal10), top = 15)
```

# Applying the model to the test set

Finally, the model is applied to predict the data in the test set.

```{r}
predict(mod.RF, newdata = testset)
```

# References and Future work

* The data used are part of the [WLE dataset](http://groupware.les.inf.puc-rio.br/har )
* For the Random Forest tuning with caret I read [James Brownlee blog page] (https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/)
* A very helpful resource for this project has been [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson (both book and related blog)

I will keep the project updated on the GitHub repository as I will use the proposed assignment to perform further analysis (e.g. PCA and prediction based on temporal series). 
